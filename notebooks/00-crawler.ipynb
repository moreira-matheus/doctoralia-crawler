{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import scrapy\n",
    "\n",
    "from ast import literal_eval\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "\n",
    "from scrapy import signals\n",
    "from scrapy.exceptions import DropItem\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.exporters import CsvItemExporter\n",
    "\n",
    "from pydispatch import dispatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation for `re` module [here](https://docs.python.org/3/library/re.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(st):\n",
    "    '''\n",
    "    Receives a string.\n",
    "    Removes \\n, \\t, commas and semicolons and strips blankspaces.\n",
    "    Returns the clean string\n",
    "    '''\n",
    "    if st is not None:\n",
    "        s = st.strip()\n",
    "        return re.sub(r'\\n|\\t|\\,|\\;', '', str(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Search info**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://www.doctoralia.com.br'\n",
    "search_url = base_url + '/pesquisa?filters%5Bspecializations%5D%5B%5D={0}&q={1}&loc={2}'\n",
    "place = 'Bahia'\n",
    "max_pages = 500\n",
    "\n",
    "spec_key, spec_value = 'specialization.id', 'specialization.name'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I/O info**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = '../data/doctoralia_raw.csv'\n",
    "driver_file = '../chromedriver.exe'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`scrapy` info**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_list = ['nome', 'endereco', 'telefone', 'especialidade', 'nota', 'registro']\n",
    "\n",
    "settings = {\n",
    "    'ITEM_PIPELINES': {'__main__.DoctoraliaPipeline': 0},\n",
    "    'FEED_EXPORT_FIELDS': fields_list\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More on `scrapy.Item`s [here](https://docs.scrapy.org/en/latest/topics/items.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoctoraliaItem(scrapy.Item):\n",
    "\n",
    "    fields = {f: scrapy.Field() for f in fields_list}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More on `scrapy` item pipelines [here](https://docs.scrapy.org/en/latest/topics/item-pipeline.html).\n",
    "- Documentation for `pydispatch` module [here](https://pypi.org/project/PyDispatcher/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoctoraliaPipeline(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        dispatcher.connect(self.spider_opened, signals.spider_opened)\n",
    "        dispatcher.connect(self.spider_closed, signals.spider_closed)\n",
    "        \n",
    "    def spider_opened(self, spider):\n",
    "        self.file = open(output_file, 'w+b')\n",
    "        self.exporter = CsvItemExporter(file=self.file, join_multivalued=',', **{'delimiter': ';'})\n",
    "        self.exporter.start_exporting()\n",
    "    \n",
    "    def spider_closed(self, spider):\n",
    "        self.exporter.finish_exporting()\n",
    "        self.file.close()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        if item['nome'] is None:\n",
    "            raise DropItem('Empty-named item.')\n",
    "\n",
    "        item['nome'] = clean_string(item['nome'])\n",
    "        \n",
    "        item['endereco'] = list(set([clean_string(end) for end in item['endereco']]))\n",
    "\n",
    "        item['telefone'] = list(set([clean_string(tel) for tel in item['telefone']]))\n",
    "\n",
    "        item['registro'] = clean_string(item['registro'])\n",
    "\n",
    "        item['especialidade'] = [clean_string(spec) for spec in item['especialidade']]\n",
    "        # Specific case: removing \"(descricao)\" specialties\n",
    "        item['especialidade'] = list(set([re.sub(r'\\(desc.+\\)', '', str(esp)) for esp in item['especialidade']]))\n",
    "        # Filtering blank specialties\n",
    "        item['especialidade'] = list(filter(None, item['especialidade']))\n",
    "\n",
    "        item = DoctoraliaItem(item)\n",
    "        self.exporter.export_item(item)\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More on `scrapy.Spider`s [here](https://docs.scrapy.org/en/latest/topics/spiders.html).\n",
    "- Documentation for `ast` module [here](https://docs.python.org/3/library/ast.html).\n",
    "- (Unofficial) Documentation for `selenium` package [here](https://selenium-python.readthedocs.io/).\n",
    "- Documentation for `bs4` (Beautiful Soup) library [here](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoctoraliaSpider(scrapy.Spider):\n",
    "\n",
    "    name = 'doctoralia'\n",
    "    start_urls = [base_url]\n",
    "\n",
    "    def __init__(self):\n",
    "        self.options = webdriver.ChromeOptions()\n",
    "        self.options.add_argument('--headless')\n",
    "\n",
    "    def start_requests(self):\n",
    "        yield scrapy.http.Request(url=self.start_urls[0],\n",
    "                                  callback=self.parse)\n",
    "    \n",
    "    def parse(self, response):\n",
    "        # Retrieves (string) list of specialties and parses it to a Python list\n",
    "        str_list = response.xpath('//template[@data-template=\"search-autocomplete-specializations\"]/@data-json').get()\n",
    "        spec_list = literal_eval(str_list)\n",
    "\n",
    "        # Opens browser\n",
    "        self.browser = webdriver.Chrome(driver_file, options=self.options)\n",
    "        \n",
    "        # Iterates over list of specialties\n",
    "        for ind, spec in enumerate(spec_list):\n",
    "            url = search_url.format(spec[spec_key], spec[spec_value].replace(' ', '+'), place)\n",
    "            \n",
    "            # (Dinamically) Iterates over pages of results\n",
    "            for page in range(1, max_pages+1):\n",
    "                self.browser.get(url)\n",
    "                soup = BeautifulSoup(self.browser.page_source)\n",
    "                \n",
    "                # Visits each result's particular page\n",
    "                for link in soup.select('a[itemprop=\"name\"]'):\n",
    "                    yield scrapy.http.Request(url=link.get('href'), callback=self.parse_result)\n",
    "                \n",
    "                # Updates URL with next page to visit (if any)\n",
    "                if bool(soup.select('li.next > a')):\n",
    "                    url = soup.select_one('li.next > a')['href']\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "        # Closes browser\n",
    "        self.browser.close()\n",
    "\n",
    "    def parse_result(self, response):\n",
    "        item = {}\n",
    "        \n",
    "        # Structuring item...\n",
    "        nome = response.xpath('//div[@class=\"unified-doctor-header-info__name\"]/span[@itemprop=\"name\"]/text()') \\\n",
    "                       .extract_first()\n",
    "        item['nome'] = nome\n",
    "        \n",
    "        enderecos = response.xpath('//span[@itemprop=\"streetAddress\"]/span[@class=\"street\"]/text()') \\\n",
    "                            .extract()\n",
    "        logradouros = [', '.join(tag.xpath('./a/text()').extract()) \\\n",
    "                       for tag in response.xpath('//span[@itemprop=\"streetAddress\"]')]\n",
    "        item['endereco'] = [', '.join(end_log) for end_log in zip(enderecos, logradouros)]\n",
    "\n",
    "        telefone = response.xpath('//div[@class=\"display-flex\"]/a[@class=\"text-muted padding-left-2\"]/b/text()') \\\n",
    "                   .extract()\n",
    "        item['telefone'] = telefone\n",
    "        \n",
    "        especialidade = response.xpath('//span[@class=\"text-ellipsis\"]/a/text()').extract()\n",
    "        item['especialidade'] = especialidade\n",
    "        \n",
    "        registro = response.xpath('//div[@class=\"text-muted small\"]/div/text()').extract_first()\n",
    "        item['registro'] = registro\n",
    "        \n",
    "        nota = response.xpath('//span[@class=\"rating rating--lg\"]/@data-score').extract_first()\n",
    "        item['nota'] = nota\n",
    "        \n",
    "        yield item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running spider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "process = CrawlerProcess(settings)\n",
    "process.crawl(DoctoraliaSpider)\n",
    "process.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
